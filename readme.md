# ğŸ¤– GenAI Knowledge Chatbot â€“ Context-Aware RAG System

A **Retrieval-Augmented Generation (RAG)** powered chatbot that uses **Google Gemini**, **LangChain**, and **Pinecone Vector Database** to deliver **context-aware answers** directly from your uploaded documents (like PDFs).  
Built with **Node.js**, this system intelligently rewrites queries, retrieves the most relevant context chunks, and generates accurate, fact-grounded responses.

---

## ğŸš€ Features

âœ… **Retrieval-Augmented Generation (RAG)** â€“ Combines retrieval + generation to improve factual accuracy  
âœ… **Google Gemini Integration** â€“ Leverages Gemini 2.0 Flash and Generative AI Embeddings for both context understanding and query rewriting  
âœ… **Vector Database Search** â€“ Uses **Pinecone** for lightning-fast semantic retrieval  
âœ… **PDF Knowledge Ingestion** â€“ Extracts, chunks, and embeds documents using **LangChain** tools  
âœ… **Query Rewriting Engine** â€“ Transforms follow-up questions into independent, contextually complete queries  
âœ… **Context-Aware Response Generation** â€“ Answers are derived solely from document context to minimize hallucinations  
âœ… **Fully Modular Node.js Pipeline** â€“ Clean, extensible design for future model or DB integrations

---

## ğŸ§  Tech Stack

| Component         | Technology                                             |
| ----------------- | ------------------------------------------------------ |
| **Language**      | Node.js (ES Modules)                                   |
| **LLM**           | Google Gemini 2.0 Flash                                |
| **Embeddings**    | Google Generative AI Embeddings (`text-embedding-004`) |
| **Vector DB**     | Pinecone                                               |
| **Frameworks**    | LangChain                                              |
| **Data Handling** | RecursiveCharacterTextSplitter, PDFLoader              |
| **Environment**   | dotenv, readline-sync                                  |

---

## ğŸ“ Project Structure

ğŸ“¦ genai-rag-chatbot
â”‚
â”œâ”€â”€ index.js # Indexes and stores the document chunks in Pinecone
â”œâ”€â”€ query.js # Handles query rewriting, retrieval, and response generation
â”œâ”€â”€ dsa.pdf # Example PDF used for knowledge ingestion
â”œâ”€â”€ .env # Environment variables (API keys, Pinecone details)
â””â”€â”€ package.json

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/GenAI-RAG-Chatbot.git
cd GenAI-RAG-Chatbot
```

## 2ï¸âƒ£ Install Dependencies

```bash
npm install
```

## 3ï¸âƒ£ Add Environment Variables

```bash
GEMINI_API_KEY=your_gemini_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=your_index_name
PINECONE_ENVIRONMENT=us-east-1
```

## 4ï¸âƒ£ Index Your PDF

Replace the sample dsa.pdf with your own document and run:

```bash
node index.js
```

This will:

Load and chunk the PDF

Generate embeddings using Gemini

Store vectors in your Pinecone index

## 5ï¸âƒ£ Start the Chat Interface

```bash
node query.js
```

Youâ€™ll be prompted to ask questions.
Each query is rewritten, semantically searched in Pinecone, and answered based only on the retrieved context.

## ğŸ§© How It Works

### ğŸ”¹ Step 1: Document Indexing (`index.js`)

- ğŸ“„ Loads a PDF using **PDFLoader**
- âœ‚ï¸ Splits content into chunks using **RecursiveCharacterTextSplitter**
- ğŸ§  Converts each chunk into embeddings using **Google Generative AI Embeddings**
- ğŸ“¦ Stores these embeddings in a **Pinecone Vector Index**

---

### ğŸ”¹ Step 2: Query Flow (`query.js`)

1. ğŸ’¬ User asks a question in the console
2. ğŸª„ The question is **rewritten by Gemini 2.0 Flash** into a standalone query
3. ğŸ” The rewritten query is **embedded and searched** in **Pinecone**
4. ğŸ“š The top retrieved chunks form the **context**
5. ğŸ’¡ **Gemini generates a response** based only on that context
6. ğŸ§¾ The response is displayed neatly in the console

---

## ğŸ§  Example Interaction

```bash
Ask me anything--> What is a binary tree?
Answer:
A binary tree is a data structure where each node has at most two children, commonly referred to as the left and right child.
Ask me anything--> And what is its time complexity for traversal?
Answer:
The time complexity for traversing a binary tree (inorder, preorder, or postorder) is O(n), where n is the number of nodes.



 User Question
      â”‚
      â–¼
ğŸ” Query Rewriting (Gemini 2.0 Flash)
      â”‚
      â–¼
ğŸ” Semantic Search (Pinecone Vector DB)
      â”‚
      â–¼
ğŸ“š Retrieve Contextual Chunks
      â”‚
      â–¼
ğŸ’¡ Gemini Response Generation
      â”‚
      â–¼
ğŸ§¾ Answer Displayed to User



## â­ Acknowledgements

- [LangChain](https://js.langchain.com/docs/)
- [Pinecone](https://www.pinecone.io/)
- [Google Gemini](https://ai.google.dev/gemini-api)
- [Node.js](https://nodejs.org/)

---

> âš¡ *â€œKnowledge grounded in context â€” powered by Generative AI.â€*
```
